## 特征选择

基本思想：去除掉**无关特征**和**冗余特征**。

> 当然有些冗余特征使学习任务更加简单，可以保留，比如：已知了“底面宽”，“底面长”，如果还有一个特征是“底面面积”，那么这个特征虽然是冗余的，但是对于要估算样本的体积，则会用到这个特征，即某些冗余特征是学习任务所需的“中间概念”，则该特征是有效的。

### 1.子集搜索

#### 1.1 前向搜索

给定特征集合$$\{ {a_1},{a_2},...,{a_n}\} $$将每一个特征看做一个候选子集，对这n个候选单特征子集进行评价，假定$$\{ {a_2}\} $$最优，那么将$$\{ {a_2}\} $$作为第一轮选定集，然后在上一轮的结果中加入一个结合，组成$$\{ {a_2},{a_1}\} $$，$$\{ {a_2},{a_3}\} $$，$$\{ {a_2},{a_4}\} $$，等n-1个候选子集。假定$$\{ {a_2},{a_4}\} $$最优，且优于$$\{ {a_2}\} $$，于是将$$\{ {a_2},{a_4}\} $$做为这一轮的选定集，......  假定在$$k + 1$$轮时，最优的候选（$$k + 1$$）特征子集不如上一轮，那么停止算法。

#### 1.2 后向搜索

类似于上面，每次从完整的特征集合中去掉一个无关特征，这样逐渐减少特征集合。

#### 1.3 前向后向结合

“双向”搜索

**上面方法中提到的评价方法**：

给定数据集D，假设D中第$$i$$类样本所占的比例为$${p_i}(i = 1,2,....,\eta )$$，假定样本属性均为离散型，对于属性子集A，假定根据其取值将D分成了V个子集$${\text{\{ }}{{\text{D}}^1}{\text{,}}{{\text{D}}^2}{\text{,}}...,{{\text{D}}^V}{\text{\} }}$$,每个子集中样本在A上的取值相同，于是可以计算属性子集A的信息增益：

$${\text{Gain(A) = Ent(D) - }}\sum\limits_{v = 1}^V {\frac{{|{D^v}|}}{{|D|}}Ent({D^v})} $$

其中信息熵是度量样本集合纯度的一个指标：

$$Ent(D) =  - \sum\limits_{i = 1}^\eta {{p_i}{{\log }_2}{p_i}} $$

信息增益越大，意味着特征子集A包含的有助于分类的信息越多。

**信息增益的解释**：

信息熵的公式如下：

![20170805200400349](.\特征选择.assets/20170805200400349-1533273794606.jpg)

对于事件B，在事件A发生情况下的条件信息熵为：

![20170805200424189](.\特征选择.assets/20170805200424189.jpg)

信息增益的公式如下，即属性A的发生与否，对于时间B的影响增量：

![20170805200443289](.\特征选择.assets/20170805200443289.jpg)



> 评价方法的背后原理：实际上特征子集A确定了数据集D的一个划分，而数据集的标记信息Y是样本的真实划分。通过估算这两个划分的差异，就能对特征子集A进行评价。与Y对应的划分差异越小，则说明A越好。信息熵只是判断这个差异的一种途径，其他能判断这两个划分差异的机制都能用于特征子集的评价。

### 2. 过滤式选择

依然只考虑样本之间的差异性，不考虑后面学习器的选择。

